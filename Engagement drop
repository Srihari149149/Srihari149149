# STEP 1: Imports
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt

# STEP 2: Load Data
# Upload your actual Excel/CSV instead of this placeholder
df = pd.read_csv("your_file.csv")

# STEP 3: Preprocess
df.rename(columns={"value": "sales_value"}, inplace=True)
df["date"] = pd.to_datetime(df["date"])
df["month"] = df["date"].dt.to_period("M")

# STEP 4: Monthly Aggregation (Dealer level)
monthly_summary = df.groupby(["dealer_id", "month"]).agg({
    "sales_value": "sum",
    "volume": "sum"
}).reset_index()

monthly_summary.sort_values(by=["dealer_id", "month"], inplace=True)
monthly_summary["value_pct_change"] = monthly_summary.groupby("dealer_id")["sales_value"].pct_change()
monthly_summary["volume_pct_change"] = monthly_summary.groupby("dealer_id")["volume"].pct_change()

# STEP 5: Detect Drops
drop_threshold = -0.3  # You can make this dynamic later
monthly_summary["value_drop_flag"] = monthly_summary["value_pct_change"] < drop_threshold
monthly_summary["volume_drop_flag"] = monthly_summary["volume_pct_change"] < drop_threshold

dealer_drops = monthly_summary[
    (monthly_summary["value_drop_flag"]) | (monthly_summary["volume_drop_flag"])
].copy()
dealer_drops["notification_type"] = "Dealer Engagement Drop"

# STEP 6: Product Category-Level Analysis
category_summary = df.groupby(["dealer_id", "month", "product_category"]).agg({
    "sales_value": "sum",
    "volume": "sum"
}).reset_index()

category_summary.sort_values(by=["dealer_id", "product_category", "month"], inplace=True)
category_summary["value_pct_change"] = category_summary.groupby(["dealer_id", "product_category"])["sales_value"].pct_change()
category_summary["volume_pct_change"] = category_summary.groupby(["dealer_id", "product_category"])["volume"].pct_change()

category_summary["value_drop_flag"] = category_summary["value_pct_change"] < drop_threshold
category_summary["volume_drop_flag"] = category_summary["volume_pct_change"] < drop_threshold

category_drops = category_summary[
    (category_summary["value_drop_flag"]) | (category_summary["volume_drop_flag"])
].copy()
category_drops["notification_type"] = "Product Category Engagement Drop"

# STEP 7: Combine All Notifications
final_notifications = pd.concat([
    dealer_drops[["dealer_id", "month", "sales_value", "volume", "value_pct_change", "volume_pct_change", "notification_type"]],
    category_drops[["dealer_id", "month", "product_category", "sales_value", "volume", "value_pct_change", "volume_pct_change", "notification_type"]]
], ignore_index=True)

# STEP 8: Output Notifications
print("🔔 Sudden Drop Notifications:")
display(final_notifications)

# Optional: Save to CSV for dashboard/emailing
final_notifications.to_csv("notifications.csv", index=False)


# Group by dealer and notification type
grouped_notifications = final_notifications.groupby(
    ['dealer code', 'notification_type']
).agg({
    'month': 'first',
    'sales_value': 'sum',
    'volume': 'sum',
    'value_pct_change': 'mean',
    'volume_pct_change': 'mean',
    'product category': lambda x: ', '.join(sorted(set(x)))  # Combine product categories
}).reset_index()

# Rename for clarity
grouped_notifications.rename(columns={
    'product category': 'affected_product_categories'
}, inplace=True)

# Show the final grouped notifications
print("Grouped Sudden Drop Notifications:")
display(grouped_notifications)

# Step: Create Custom Notification Messages
grouped_notifications['notification_message'] = grouped_notifications.apply(
    lambda row: (
        f"ALERT: Sales value/volume dropped for dealer {row['dealer code']} in {row['month']}. "
        f"Affected categories: {row['affected_product_categories']}. "
        f"Value dropped by {row['value_pct_change']:.1%}, volume dropped by {row['volume_pct_change']:.1%}. "
        f"Please investigate and take corrective action."
    ), axis=1
)

# Display Final Output
display(grouped_notifications[['dealer code', 'notification_message']])

# Group by month and notification_type to summarize dealers and product categories
summary_notifications = grouped_notifications.groupby(
    ['month', 'notification_type']
).agg({
    'dealer code': lambda x: ', '.join(sorted(set(map(str, x)))),
    'product category': lambda x: ', '.join(sorted(set(x)))
}).reset_index()

# Format and generate the final message
summary_notifications['summary_message'] = summary_notifications.apply(
    lambda row: (
        f"🔔 {row['notification_type']} for {row['product category']} in {row['month']}.\n"
        f"Affected dealers: {row['dealer code']}.\n"
        f"Sales value or volume dropped by over 30%. Please visit these dealers and investigate low engagement."
    ), axis=1
)

# Display the output
display(summary_notifications[['month', 'notification_type', 'summary_message']])


PREDICTIVE PART

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# STEP 0: Sort and prepare your existing data
df = df.sort_values(['dealer code', 'product category', 'month'])

df = df.groupby(['dealer code', 'product category', 'month'], as_index=False).agg({
    'sales_value': 'sum',
    'volume': 'sum'
})

# STEP 1: Feature Engineering
df['value_lag1'] = df.groupby(['dealer code', 'product category'])['sales_value'].shift(1)
df['volume_lag1'] = df.groupby(['dealer code', 'product category'])['volume'].shift(1)

df['value_pct_change'] = (df['sales_value'] - df['value_lag1']) / df['value_lag1']
df['volume_pct_change'] = (df['volume'] - df['volume_lag1']) / df['volume_lag1']

df['value_roll_mean_2'] = df.groupby(['dealer code', 'product category'])['sales_value'].transform(
    lambda x: x.shift(1).rolling(2).mean())
df['volume_roll_mean_2'] = df.groupby(['dealer code', 'product category'])['volume'].transform(
    lambda x: x.shift(1).rolling(2).mean())

# STEP 2: Create binary target (drop if either value or volume drops >30%)
df['engagement_drop'] = ((df['value_pct_change'] < -0.3) | (df['volume_pct_change'] < -0.3)).astype(int)

# STEP 3: Drop NA rows
ml_data = df.dropna(subset=[
    'value_lag1', 'volume_lag1',
    'value_pct_change', 'volume_pct_change',
    'value_roll_mean_2', 'volume_roll_mean_2'
])

# STEP 4: Split into train (months 1-3) and predict (month 4)
month_list = sorted(ml_data['month'].unique())
month_list = sorted(pd.to_datetime(ml_data['month'].dropna().unique()))


if len(month_list) >= 4:
    train_months = month_list[:3]
    predict_month = month_list[3]

    train_data = ml_data[ml_data['month'].isin(train_months)].copy()
    predict_data = ml_data[ml_data['month'] == predict_month].copy()

    # Check for multiple classes in target
    if train_data['engagement_drop'].nunique() < 2:
        print("❗ Training data only has one class. Try adjusting drop threshold or check data size.")
    else:
        # STEP 5: Train model
        features = [
            'value_lag1', 'volume_lag1',
            'value_pct_change', 'volume_pct_change',
            'value_roll_mean_2', 'volume_roll_mean_2'
        ]

        X_train = train_data[features]
        y_train = train_data['engagement_drop']

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)

        model = LogisticRegression()
        model.fit(X_train_scaled, y_train)

        # STEP 6: Predict
        X_predict = predict_data[features]
        X_predict_scaled = scaler.transform(X_predict)
        predict_data['drop_prob'] = model.predict_proba(X_predict_scaled)[:, 1]

        # STEP 7: Generate Notification Messages
   # Shift prediction label to next month (Month 4)
        predict_data['predicted_month'] = predict_data['month'] + pd.DateOffset(months=1)

# Generate notification message using predicted_month instead of actual data month
    predict_data['notification'] = predict_data.apply(
    lambda row: f"🔔 Product category engagement drop likely for '{row['product category']}' at dealer '{row['dealer code']}' in {row['predicted_month'].strftime('%B')}. Risk: {row['drop_prob']*100:.1f}%.",
    axis=1
)


        # STEP 8: Output
        output = predict_data[['dealer code', 'product category', 'month', 'drop_prob', 'notification']]
        print(output)

else:
    print("❗ Not enough data to train and predict. You need at least 4 months of clean data after feature creation.")


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report

# Step 1: Get your predicted labels (0 or 1) from the model
y_pred = model.predict(X_predict_scaled)  # use your test/predict features here
y_true = predict_data['engagement_drop']  # actual target values (if you have Month 3 labels)

# Step 2: Evaluate metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, zero_division=0)
recall = recall_score(y_true, y_pred, zero_division=0)
f1 = f1_score(y_true, y_pred, zero_division=0)
roc_auc = roc_auc_score(y_true, predict_data['drop_prob'])
conf_matrix = confusion_matrix(y_true, y_pred)

# Step 3: Print everything
print("✅ Logistic Regression Metrics:")
print(f"🔹 Accuracy      : {accuracy:.3f}")
print(f"🔹 Precision     : {precision:.3f}")
print(f"🔹 Recall        : {recall:.3f}")
print(f"🔹 F1 Score      : {f1:.3f}")
print(f"🔹 ROC AUC Score : {roc_auc:.3f}")
print("\n🔍 Confusion Matrix:")
print(conf_matrix)

# Optional: full classification report
print("\n📊 Classification Report:")
print(classification_report(y_true, y_pred, zero_division=0))

print("Sample month in data:", ml_data['month'].iloc[0], type(ml_data['month'].iloc[0]))
print("Sample in train_months:", train_months[0], type(train_months[0]))
print("Train months:", train_months)
print("Unique months in data:", sorted(ml_data['month'].unique()))

