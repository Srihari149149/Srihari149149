# STEP 1: Imports
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt

# STEP 2: Load Data
# Upload your actual Excel/CSV instead of this placeholder
df = pd.read_csv("your_file.csv")

# STEP 3: Preprocess
df.rename(columns={"value": "sales_value"}, inplace=True)
df["date"] = pd.to_datetime(df["date"])
df["month"] = df["date"].dt.to_period("M")

# STEP 4: Monthly Aggregation (Dealer level)
monthly_summary = df.groupby(["dealer_id", "month"]).agg({
    "sales_value": "sum",
    "volume": "sum"
}).reset_index()

monthly_summary.sort_values(by=["dealer_id", "month"], inplace=True)
monthly_summary["value_pct_change"] = monthly_summary.groupby("dealer_id")["sales_value"].pct_change()
monthly_summary["volume_pct_change"] = monthly_summary.groupby("dealer_id")["volume"].pct_change()

# STEP 5: Detect Drops
drop_threshold = -0.3  # You can make this dynamic later
monthly_summary["value_drop_flag"] = monthly_summary["value_pct_change"] < drop_threshold
monthly_summary["volume_drop_flag"] = monthly_summary["volume_pct_change"] < drop_threshold

dealer_drops = monthly_summary[
    (monthly_summary["value_drop_flag"]) | (monthly_summary["volume_drop_flag"])
].copy()
dealer_drops["notification_type"] = "Dealer Engagement Drop"

# STEP 6: Product Category-Level Analysis
category_summary = df.groupby(["dealer_id", "month", "product_category"]).agg({
    "sales_value": "sum",
    "volume": "sum"
}).reset_index()

category_summary.sort_values(by=["dealer_id", "product_category", "month"], inplace=True)
category_summary["value_pct_change"] = category_summary.groupby(["dealer_id", "product_category"])["sales_value"].pct_change()
category_summary["volume_pct_change"] = category_summary.groupby(["dealer_id", "product_category"])["volume"].pct_change()

category_summary["value_drop_flag"] = category_summary["value_pct_change"] < drop_threshold
category_summary["volume_drop_flag"] = category_summary["volume_pct_change"] < drop_threshold

category_drops = category_summary[
    (category_summary["value_drop_flag"]) | (category_summary["volume_drop_flag"])
].copy()
category_drops["notification_type"] = "Product Category Engagement Drop"

# STEP 7: Combine All Notifications
final_notifications = pd.concat([
    dealer_drops[["dealer_id", "month", "sales_value", "volume", "value_pct_change", "volume_pct_change", "notification_type"]],
    category_drops[["dealer_id", "month", "product_category", "sales_value", "volume", "value_pct_change", "volume_pct_change", "notification_type"]]
], ignore_index=True)

# STEP 8: Output Notifications
print("ðŸ”” Sudden Drop Notifications:")
display(final_notifications)

# Optional: Save to CSV for dashboard/emailing
final_notifications.to_csv("notifications.csv", index=False)


# Group by dealer and notification type
grouped_notifications = final_notifications.groupby(
    ['dealer code', 'notification_type']
).agg({
    'month': 'first',
    'sales_value': 'sum',
    'volume': 'sum',
    'value_pct_change': 'mean',
    'volume_pct_change': 'mean',
    'product category': lambda x: ', '.join(sorted(set(x)))  # Combine product categories
}).reset_index()

# Rename for clarity
grouped_notifications.rename(columns={
    'product category': 'affected_product_categories'
}, inplace=True)

# Show the final grouped notifications
print("Grouped Sudden Drop Notifications:")
display(grouped_notifications)

# Step: Create Custom Notification Messages
grouped_notifications['notification_message'] = grouped_notifications.apply(
    lambda row: (
        f"ALERT: Sales value/volume dropped for dealer {row['dealer code']} in {row['month']}. "
        f"Affected categories: {row['affected_product_categories']}. "
        f"Value dropped by {row['value_pct_change']:.1%}, volume dropped by {row['volume_pct_change']:.1%}. "
        f"Please investigate and take corrective action."
    ), axis=1
)

# Display Final Output
display(grouped_notifications[['dealer code', 'notification_message']])

# Group by month and notification_type to summarize dealers and product categories
summary_notifications = grouped_notifications.groupby(
    ['month', 'notification_type']
).agg({
    'dealer code': lambda x: ', '.join(sorted(set(map(str, x)))),
    'product category': lambda x: ', '.join(sorted(set(x)))
}).reset_index()

# Format and generate the final message
summary_notifications['summary_message'] = summary_notifications.apply(
    lambda row: (
        f"ðŸ”” {row['notification_type']} for {row['product category']} in {row['month']}.\n"
        f"Affected dealers: {row['dealer code']}.\n"
        f"Sales value or volume dropped by over 30%. Please visit these dealers and investigate low engagement."
    ), axis=1
)

# Display the output
display(summary_notifications[['month', 'notification_type', 'summary_message']])


import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Step 0: Read and prepare data
df = data.copy()  # Assume `data` is your existing DataFrame from descriptive part

# Sort and prepare
df['month'] = pd.to_datetime(df['month'])
df = df.sort_values(['dealer code', 'product category', 'month'])

# Step 1: Feature Engineering
df['value_lag1'] = df.groupby(['dealer code', 'product category'])['sales_value'].shift(1)
df['volume_lag1'] = df.groupby(['dealer code', 'product category'])['volume'].shift(1)

df['value_pct_change'] = (df['sales_value'] - df['value_lag1']) / df['value_lag1']
df['volume_pct_change'] = (df['volume'] - df['volume_lag1']) / df['volume_lag1']

df['value_roll_mean_2'] = df.groupby(['dealer code', 'product category'])['sales_value'].shift(1).rolling(2).mean()
df['volume_roll_mean_2'] = df.groupby(['dealer code', 'product category'])['volume'].shift(1).rolling(2).mean()

# Step 2: Create Binary Target Column
df['engagement_drop'] = ((df['value_pct_change'] < -0.3) | (df['volume_pct_change'] < -0.3)).astype(int)

# Drop rows with NA
ml_data = df.dropna(subset=['value_lag1', 'volume_lag1', 'value_pct_change', 'volume_pct_change',
                            'value_roll_mean_2', 'volume_roll_mean_2', 'engagement_drop'])

# Step 3: Model Training
features = ['value_lag1', 'volume_lag1', 'value_pct_change', 'volume_pct_change', 'value_roll_mean_2', 'volume_roll_mean_2']
X = ml_data[features]
y = ml_data['engagement_drop']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("ðŸ“Š Classification Report:\n", classification_report(y_test, y_pred))

# Step 4: Predict on Full Data
ml_data['drop_prob'] = model.predict_proba(X_scaled)[:, 1]

# Show top risky items
alerts = ml_data[ml_data['drop_prob'] > 0.6][['dealer code', 'product category', 'drop_prob', 'month']]

# Step 5: Generate Notification Messages
alerts['notification'] = alerts.apply(
    lambda x: f"ðŸ”” Product category engagement drop likely for '{x['product category']}' at dealer '{x['dealer code']}' in {x['month'].strftime('%B')}. Risk: {x['drop_prob']*100:.1f}%.", axis=1
)

# View final alerts
alerts = alerts.sort_values(by='drop_prob', ascending=False).reset_index(drop=True)
alerts[['dealer code', 'product category', 'drop_prob', 'notification']]


